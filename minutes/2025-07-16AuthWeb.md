# Authentic Web mini-workshop series: trust.txt

Jul 16, 2025
[Reading Materials](https://github.com/w3c/authentic-web-workshop/issues/27)
[Video Record](https://customer-0kix77mxh2zzzae0.cloudflarestream.com/b03295ffe3a1cd8689fc79824f9d5311/watch)

Chat: irc.w3.org #CredWeb
Chair: [Tzviya Siegman](mailto:tzviya@w3.org)
Scribe: [Dominique Hazael-Massieux](mailto:dom@w3.org)

Present:
- Dominique Hazael-Massieux (W3C)
- Tzviya Siegman (W3C)
- Barrett Golding (Iffy.news)
- Sandro Hawke
- Scott Yates
- Ehsan Toreini (Samsung)
- Frank Michel
- Michiko Kuriyama (OPCIP)
- Ralph Brown
- Mike Pennisi
- Ted Thibodeau
- Daveed Benjamin
- Bob Wyman
- Franck Michel
- Pierre-Antoine Champin
- Jennifer Strickland
- Annette Greiner
- Owen Ambur
- Chris Needham
- Chris Rusnak
- Chris Lombardi
- Max Gendler (NewsCorp, AB)
- Jeffrey Yasskin (Google Chrome, TAG)
- Christy Tanner

Scott: I'm the founder and interim executive director of JournalList; some background on trust.txt - if you read the news and want to check whether what you're reading is from a credible source. If you go to [apnews.com](http://apnews.com), want to check about them, you go to their trust.txt file, which links to [ap.org](http://ap.org), whose trust.txt confirms they control [apnews.com](http://apnews.com), and that they belong to [journalist.net](http://journalist.net) and [iptc.org](http://iptc.org) associations. [iptc.org](http://iptc.org) has also a trust.txt that confirms that [ap.org](http://ap.org) is a member of IPTC. All these relationships are machine readable, using a format similar to what crawler bots use. The trust.txt has been out there for a while, and builds a graph of relationship between web sites that a crawler (e.g. a search engine) can follow and collect.

It can also be used by marketers - e.g. audited media, a group that helps marketers how many views and how reputable a given site is, has a tool to figure out how much money to send on a media web site - "media signal select". One of the signals they use is whether they have a trust.txt file; next they'll consider association membership as another signal

A recent piece of news: taking [grahamleader.com](http://grahamleader.com) as an example of local news site, close to the community they're covering, and which surveys have shown readers have more confidence in (while their business model is more fragile in the Web ecosystem and they may not have technical expertise for SEO) - they have a trust.txt file that shows they're a member of [texaspress.com](http://texaspress.com). They're now able to display that in their web page with a trust://[texaspress.com](http://texaspress.com) URL, which gets verified on [texaspress.com](http://texaspress.com) by a Trust URI extension which then shows a "trust" badge on the page (with a link to an explanation). This is particularly helpful in a world where so much AI content is hard to identify from trustworthy content.

This can also be used in a social media context to verify that a given social profile is recognized by the [grahamleader.com](http://grahamleader.com) as theirs.

Trust.txt doesn't make any claim about content, only about relationships.

This can be used by organizations, but also by individuals - e.g. on my linkedin profile.

The extension shows a red badge if there is a mismatch between the trust:// URL and what the trust.txt describes.

I'm interested in having this become a web standard: it's interoperable, as secure as your web domain, based on fairness and royalty-free, has low technical requirements without requiring PKI knowledge.

Ted: how does this protect against story-level hackage? As long as everything is not vulnerable, this seems fine; but as soon as one element of the chain is attacked, it falls apart. E.g. domain content, domain name registration may not be secured

Scott: if you don't control your URL space, you have bigger problems than your trust.txt file; in my experience wrt media publishers and security, I haven't seen stories of active publishers losing control of their own domain; publishers becoming inactive may have their domains taken over by scammers.

Ralph: quick intro: I'm a Board member of JournaList, long time in the telecom industry. If you're going to attack a publisher domain and take control of it, you'll be looking immediately at how to alter the content and how to take advantage of the site - trust.txt wouldn't' be a prime avenue to take advantage of this type of things

Dom: It might be worth exploring publishers who are particularly vulnerable to attacks because they are not very technical. It can be hard to detect the scam because of the trust.txt chain of trust that exists.

Scott: I could only wish for trust.txt to gain so much traction that it would be a primary target; if it would be easy to detect with a change tracker that alerts you when a change happen.

Jeffrey: the ability to compromise one of these organizations to include an hostile site is real - but the system provides value despite that risk; knowing which web sites are associated - there are existing other systems link rel=me (https://microformats.org/wiki/rel-me#domain_verification), associations between app and domains (https://developers.google.com/digital-asset-links/v1/getting-started and https://developer.apple.com/documentation/xcode/supporting-associated-domains) - they have the same risk of attacks, but they still provide value to declare these associations

Tzviya: we should add this to threat model, and maybe move on

Ralph: we have considered using signed trust.txt, but we favor adoption and deployment; if trust.txt became a primary target, we could add signature as a later development

ChrisN: the local news industry has been severely reduced in its ability to reach its audiences; the growth of the Web and advertising revenue moving to search engines is something we should be cognizant of. I would like us to move forward with addressing this space. Not all publishers would be comfortable with disclosing all their memberships and relationships - e.g. BBC wouldn't want to associate authoritativeness to its memberships of other organizations when we think our history speaks for itself; but it feels like a good tool for smaller publishers.

Scott: this would still be applicable and useful for social accounts

Chris: absolutely - it would remove the dependency on verified accounts of these social platforms

Scott: we also have an AI variable to say whether AI training would be allowed or not

Max: +1 to ChrisN re large publishers (based on my experience working at two of these); I have a concern on trust.txt from a publisher perspective: we see voluntary disclosures twisted in the market place, where publishers get pressured to add or remove some of them, e.g. in the context of advertising or search (e.g. in the context of robots.txt or ads.txt). I'm not sure what's the mitigation would be - although it's two-sided declaration, so maybe that reduces the risk a bit.

Scott: a trust.txt file that would only list social accounts doesn't make any grand declaration, it's just a machine readable file of already public information.

Max: imagine an adtech partner coming to us complaining that our trust.txt isn't complete because we only list our socials and threaten to demonetize us (for some reason) - this would be problematic; what's the larger ecosystem might start mandating because of market forces.

Ted: re using a content monitor to detect changes in trust.txt - that requires more tech-savvyness than what was described as the target adopter. The little red badge is not very noticeable, esp for people with difficulty detect color differences - something more prominent would be necessary.

Scott: if this was integrated in browser, I would assume it would get better and adaptable UI.

Tzviya: there are questions about where this lives, questions about relationship with AI Prefs in IETF; would also be interested to hear about your thoughts about the role of W3C in this.

Scott: in terms of how this work with other systems - it could be very useful for C2PA; they need a list of trusted publishers associated with a piece of content. IPTC and C2PA do that with pretty heavy certification and processes to get added to a list, which would work for big publishers but definitely not for smaller ones. With trust.txt, instead of trusting only certified publishers, you could use a lighter-weight process. Trust.txt is meant to declare claims that are neither private nor controversial.

Sandro: trust.txt could actually be generated by an LLM at this point, based on natural language content of the Web site

Tzviya: our next meeting will be on Originator Profile on September 10 - invitation upcoming shortly; we're also to have either a virtual or physical workshop to figure out how to put all of these things together into a standardization roadmap.
